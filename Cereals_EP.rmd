---
title: "Cereal EP"
author: "Alex Bystrov, Rocio Rodriguez, Bodam Jerry"
date: "2025-02-18"
output: cereal_filtered_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r cars}

#Initial setup

rm(list=ls())

if (!requireNamespace("pacman", quietly = TRUE)) {
  install.packages("pacman")
}

pacman::p_load(
  fixest, haven, modelsummary, tidyverse, readxl, dplyr, plm, stargazer, IDPmisc, lfe, here, sandwich, AER, 
  dynlm, forecast, scales, quantmod, urca, nlme, lmtest, pdynmc, dynpanel, rstudioapi, pgmm, xtable, estimatr, 
  zoo, skimr, lubridate, patchwork, tidyr, data.table, ggplot2, car, here, tibble
)

# Get the directory path of the current R Markdown file
current_rmd <- rstudioapi::getSourceEditorContext()$path

# Set the working directory to the location of the R Markdown file
setwd(dirname(current_rmd))

data <- read.csv(file.path(getwd(), "products.csv"), sep = ",")
```

# 1. A first look at the data. 

```{r pressure, echo=FALSE}
print(head(data))

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

Q1. Describe the data
Using products.csv, compute summary statistics for different the different variables

Q1. Summary stats II

```{r}
cereal_data <- data
cereal_data$market <- as.factor(cereal_data$market)
cereal_data$product <- as.factor(cereal_data$product)
cereal_data$mushy <- as.factor(cereal_data$mushy)

summary(cereal_data)


y = log(cereal_data$servings_sold)

ggplot(cereal_data, aes(x = price_per_serving, y = log(servings_sold))) +
  geom_point() +  # Scatter points
  geom_smooth(method = "lm", se = FALSE, color = "red") + # Regression line (linear model)
  labs(title = "Scatter Plot with Regression Line",
       subtitle = "Servings Sold vs. Price per Serving",
       x = "Price per Serving",
       y = "Log(Servings Sold)") +
  theme_minimal()

```

```{r}
#Feature engineering

cereal_data$firm_id <- substr(cereal_data$product, 2, 2)
cereal_data$firm_id <- as.factor(cereal_data$firm_id)

cereal_data$market_id <- substr(cereal_data$market, 1, 3)
cereal_data$market_id <- as.factor(cereal_data$market_id)

cereal_data$quarter <- stringr::str_sub(cereal_data$market, start = -2) # Negative start index counts from the end
cereal_data$quarter <- as.factor(cereal_data$quarter)


```

```{r}
ggplot(cereal_data, aes(x = price_per_serving, y = y, color = factor(market_id))) +
  geom_point() +  # Scatter points, color-coded by market id
  geom_smooth(method = "lm", se = FALSE, color = "red") + # Regression line (overall, in red)
  labs(title = "Scatter Plot with Regression Line",
       subtitle = "Servings Sold (Log) vs. Price per Serving, Colored by Market ID",
       x = "Price per Serving",
       y = "Log(Servings Sold)",
       color = "Market ID") + # Label for the color legend
  theme_minimal() +
  scale_color_discrete(name = "Market ID") 

```


```{r}
ggplot(cereal_data, aes(x = price_per_serving, y = y, color = factor(quarter))) +
  geom_point() +  # Scatter points, color-coded by quarter id
  geom_smooth(method = "lm", se = FALSE, color = "red", aes(linetype = factor(quarter))) +  # Regression lines
  labs(title = "Scatter Plot with Regression Line",
       subtitle = "Servings Sold (Log) vs. Price per Serving, Colored by Quarter ID",
       x = "Price per Serving",
       y = "Log(Servings Sold)",
       color = "Quarter ID") + # Label for the color legend
  theme_minimal() +
  scale_color_discrete(name = "Quarter ID") 

```

```{r}
ggplot(cereal_data, aes(x = price_per_serving, y = y, color = factor(firm_id))) +
  geom_point() +  # Scatter points, color-coded by firm_id
  geom_smooth(method = "lm", se = FALSE, color = "red") + # Regression line (overall, in red)
  labs(title = "Scatter Plot with Regression Line",
       subtitle = "Servings Sold (Log) vs. Price per Serving, Colored by Firm",
       x = "Price per Serving",
       y = "Log(Servings Sold)",
       color = "Firm ID") + # Label for the color legend
  theme_minimal() +
  scale_color_discrete(name = "Firm ID")
```
```{r}

ggplot(cereal_data, aes(x = price_per_serving, y = y, color = factor(mushy))) +
  geom_point() +  # Scatter points, color-coded by mushy
  geom_smooth(method = "lm", se = FALSE, color = "red") + # Regression line (overall, in red)
  labs(title = "Scatter Plot with Regression Line",
       subtitle = "Servings Sold (Log) vs. Price per Serving, Colored by Mushy",
       x = "Price per Serving",
       y = "Log(Servings Sold)",
       color = "Mushy") + # Label for the color legend
  theme_minimal() +
  scale_color_discrete(name = "Mushy")
```



Alex's:

```{r}
summary(cereal_data)

```

```{r}

y = log(cereal_data$servings_sold)

ggplot(cereal_data, aes(x = log(servings_sold), y = price_per_serving)) +
  geom_point() +  # Scatter points
  geom_smooth(method = "lm", se = FALSE, color = "red") + # Regression line (linear model)
  labs(title = "Scatter Plot with Regression Line",
       subtitle = "Servings Sold vs. Price per Serving",
       x = "Log(Servings Sold)",
       y = "Price per Serving") +
  theme_minimal()
```

The dataset under analysis is a panel dataset, as it consists of repeated observations over time. Each unit represents a specific product sold by a firm in a particular market at a given point in time (quarterly observations). The dataset includes multiple attributes such as price, quantities sold, product characteristics (e.g., mushy or crunchy), and a time identifier (quarter). What can be noted so far is that: no missing values are present, ensuring data completeness. The distributions of ‘servings_sold’ and ‘city_population’ appear to be highly skewed, likely following a power-law distribution pattern. We can also see the negative slope of the regression line, which is in line with economic theory, meaning that demand and price are inversely proportional. 

```{r}


par(mfrow = c(1,4), mar = c(4, 4, 5, 1), oma = c(1, 0, 3, 0))  # oma reserves space for the title

# 1st plot: Distribution of Servings Sold
hist(data$servings_sold, breaks = 30, col = "lightblue", border = "black",
     main = "Servings Sold", xlab = "Servings Sold")

# 2nd plot: Distribution of City Population
hist(data$city_population, breaks = 30, col = "lightcoral", border = "black",
     main = "City Population", xlab = "City Population")

# 3rd plot: Distribution of Price per Serving
hist(data$price_per_serving, breaks = 30, col = "lightgreen", border = "black",
     main = "Price per Serving", xlab = "Price per Serving",cex = 1.5, font = 2)

# 4th plot: Distribution of Price Instrument
hist(data$price_instrument, breaks = 30, col = "lightgoldenrod", border = "black",
     main = "Price Instrument", xlab = "Price Instrument")

# Add main title to the entire panel
mtext("Distributions of Continuous Variables", outer = TRUE, cex = 1.5, font = 2)

```

From the figure "Distributions of Continuous Variables" we can see that the distributions of ‘servings_sold’ and ‘city_population’ indeed exhibit characteristics of a power-law distribution. This is expected for city populations, as urban populations typically follow a power-law pattern due to underlying socio-economic and geographical factors. Given that ‘servings sold’ is directly influenced by city population, it is likely that these two variables are highly correlated.
To ensure a more accurate representation in future demand visualizations, log-transforming ‘servings sold’ is necessary. This transformation helps linearize relationships, mitigate skewness, and enhance interpretability in regression models.
In contrast, ‘price per serving’ and ‘price instrument’ appear to follow a normal distribution. This suggests that pricing decisions and instrumental variables are likely centered around a mean value with symmetric dispersion, which aligns with standard market mechanisms.

```{r}
cor_data <- cereal_data[, c("servings_sold", "city_population", "price_per_serving", "price_instrument")]

# Compute the correlation matrix
cor_matrix <- cor(cor_data, use = "complete.obs")  # Use complete observations only

# Print the correlation matrix
cor_matrix

```

Correlation matrix shows 2 significant correlations: 1) between price per serving and price instrument. Which means that 1st condition (relevance) of the valid instrument variable is satisfied (Cov(X,Z)!=0); 2) between servings sold and city population is expected, as larger populations naturally generate higher total demand. These findings reinforce the appropriateness of the hypothetical instrumental variable approach and the importance of controlling for city-specific demand factors in the analysis.

```{r, echo=TRUE, fig.show='hold'}
library(dplyr)

par(mar = c(5, 4, 3, 4) + 0.3, mgp = c(2, 1, 0))
plot_count <- 0  
unique_firms <- unique(cereal_data$firm_id)

for (firm in unique_firms) {
  subset_data <- cereal_data %>% filter(firm_id == firm)  # Ensure correct type matching
  print(dim(subset_data))
  if (nrow(subset_data) > 1) {  
    subset_data$product <- as.character(subset_data$product)  
    unique_products <- unique(subset_data$product)
    product_colors <- setNames(rainbow(length(unique_products)), unique_products)  
    
    plot(log(subset_data$servings_sold), subset_data$price_per_serving,
         xlab = "Log(Servings Sold)", ylab = "Price per Serving",
         col = product_colors[subset_data$product],  
         pch = 16,
         main = paste("Firm", firm),
         cex.lab = 1.2,
         cex.main = 1.5)
    
    legend("topright", legend = unique_products, col = product_colors, pch = 16, 
           title = "Product", cex = 0.8, bty = "n")
    
    plot_count <- plot_count + 1
    
    # If this is the first plot, add title
    if (plot_count == 1) {  
      mtext("Demand plots by firm and product", outer = TRUE, cex = 2, font = 2, line = 3)
    }

    # If 6 plots are completed, reset for a new page
    if (plot_count %% 6 == 0) {  
      dev.new()  # Open a new graphics window
      par(mfrow = c(2,3), mar = c(4, 4, 2, 1), oma = c(5, 5, 5, 2))  
    }
  }
}

```

```{r}
library(dplyr)

par(mfrow = c(2,3), mar = c(4, 4, 2, 1), oma = c(5, 5, 5, 2))  # Set multi-plot layout before looping

plot_count <- 0  
unique_firms <- unique(cereal_data$firm_id)

for (firm in unique_firms) {
  subset_data <- cereal_data %>% filter(firm_id == firm, servings_sold > 0)  # Ensure no log issues
  
  if (nrow(subset_data) > 1) {  
    subset_data$product <- as.character(subset_data$product)  # Convert to character to avoid factor issues
    unique_products <- unique(subset_data$product)  
    product_colors <- setNames(rainbow(length(unique_products)), unique_products)  
    
    plot(log(subset_data$servings_sold), subset_data$price_per_serving,
         xlab = "Log(Servings Sold)", ylab = "Price per Serving",
         col = product_colors[subset_data$product],  
         pch = 16,
         main = paste("Firm", firm),
         cex.lab = 1.2,
         cex.main = 1.5)
    
    legend("topright", legend = unique_products, col = product_colors, pch = 16, 
           title = "Product", cex = 0.8, bty = "n")
    
    plot_count <- plot_count + 1
    
    if (plot_count == 1) {  
      mtext("Demand plots by firm and product", outer = TRUE, cex = 2, font = 2, line = 3)
    }

    # If 6 plots are completed, reset for a new page
    if (plot_count %% 6 == 0) {  
      dev.new()  # Open a new graphics window
      par(mfrow = c(2,3), mar = c(4, 4, 2, 1), oma = c(5, 5, 5, 2))  
    }
  }
}
```



```{r}
cereal_data_ <- cereal_data %>%
  mutate(mushy = as.numeric(mushy)) 

mushy_percentage_by_firm <- cereal_data_ %>%
  group_by(firm_id) %>%
  summarise(Mushy_Percentage = ((sum(mushy, na.rm = TRUE) / length(mushy)) - 1)  * 100)

# Print the result in R console
print(mushy_percentage_by_firm)

```
The demand plots by firm and product reveal a clear clustering of different products within each firm. This pattern likely reflects firm`s strategic efforts to vertically differentiate their products by offering varying levels of quality. Table just above suggests that three out of five firms engage in vertical differentiation, as they offer products of differing quality levels (mushy and crunchy, representing lower and higher quality, respectively). In contrast, Firm 4, which does not produce mushy cereals, appears to differentiate its products vertically within the crunchy segment, as observed in the demand plot for Firm 4.

```{r}
# Plot 3: Mushy vs Crunchy in the same plot using ggplot2
ggplot(data, aes(x = log(servings_sold), y = price_per_serving, color = factor(mushy))) +
  geom_point(alpha = 0.4) +  # Scatter plot with transparency
  geom_smooth(method = "lm", se = FALSE, aes(linetype = factor(mushy))) +  # Regression lines
  labs(
       x = "Log(Servings Sold)",
       y = "Price per Serving",
       color = "Cereal Type",
       linetype = "Cereal Type") +  # Legend for line types
  scale_color_manual(values = c("blue", "red"), labels = c("Crunchy", "Mushy")) +
  scale_linetype_manual(values = c("solid", "solid"), labels = c("Crunchy", "Mushy")) +  # Different line types
  theme(
    plot.title = element_text(size = 15, face = "bold", family = "sans"),  # Title font settings
    axis.title = element_text(size = 15, family = "sans"))+  # Adjust axis labels 
  labs(title = "Demand: Mushy vs Crunchy Cereals")
```

On the aggregated demand plot, we observe distinct slopes for the demand curves: the crunchy cereal segment exhibits a steeper slope, suggesting lower price elasticity, while the mushy cereal segment has a less steep slope, indicating higher price sensitivity. Initially, this could suggest that the mushy segment has a greater availability of substitutes or a more competitive market structure. However, an alternative explanation is that consumers in the budget-oriented mushy segment perceive it as non-essential and may switch to other food options, whereas consumers in the crunchy (higher-quality) segment exhibit greater brand loyalty and lower price sensitivity. 


The hypothesis of vertical differentiation is further supported by the demand patterns of Firm 3, in conjunction with data from table with % of mushy products per firm (above). A closer examination of Firm 3 allows for a comparison of
price elasticity between mushy and crunchy products, as reflected in the firm-specific demand plot. Notably, Product 6 (F3B06), which belongs to the mushy segment, has a noticeably less steep demand
curve, reinforcing the notion that mushy cereals are associated with higher price sensitivity. This serves
as additional descriptive evidence supporting the vertical product differentiation hypothesis.

```{r}

f3_data  <- cereal_data %>%
  filter(firm_id==3)

ggplot(f3_data, aes(x = log(servings_sold), y = price_per_serving, color = factor(mushy))) +
  geom_point(alpha = 0.6) +  # Scatter plot with transparency
  geom_smooth(method = "lm", se = FALSE, aes(linetype = factor(mushy))) +  # Regression lines
  labs(
       x = "Log(Servings Sold)",
       y = "Price per Serving",
       color = "Cereal Type",
       linetype = "Cereal Type") +  # Legend for line types
  scale_color_manual(values = c("#00e5ff", "red"), labels = c("Crunchy", "Mushy")) +
  scale_linetype_manual(values = c("solid", "solid"), labels = c("Crunchy", "Mushy")) +  # Different line types
  theme(
    plot.title = element_text(size = 15, face = "bold", family = "sans"),  # Title font settings
    axis.title = element_text(size = 10, family = "sans"))+  # Adjust axis labels 
  labs(title = "Firm 3 Demand: Mushy vs Crunchy Cereals")


```

To test the hypothesis about causes of different price elasticities in the segments we calculated: (i) the number of substitutes per segment and (ii) the degree of competition in each segment:

1. Substitutes per Segment:
  • The total number of crunchy products is 16, whereas the mushy segment consists of only 8
    products.
  • This suggests that consumers in the crunchy segment have access to more substitute options,
    which could contribute to higher price elasticity.
    
2. Market Competition Analysis (HHI):
  • We calculated market shares for firms within each segment at the city-quarter level and derived
    the Herfindahl-Hirschman Index (HHI) as a measure of market concentration.
  • The average HHI for the crunchy segment is 809, while the HHI for the mushy segment is 1593.
  • Since a higher HHI value indicates a less competitive market structure, this result suggests that
    the crunchy segment is more competitive than the mushy segment.

```{r}

## 
mushy_products <- data %>%
  filter(mushy==1)%>%
  count(product)

num_unique_mushy_products <- nrow(mushy_products)

crunchy_products <- data %>%
  filter(mushy==0)%>%
  count(product)

num_unique_crunchy_products <- nrow(crunchy_products)

print(num_unique_mushy_products)
print(num_unique_crunchy_products)

##

data_ms <- cereal_data %>%
  group_by(quarter, market_id, mushy, firm_id) %>%  
  mutate(firm_sales = sum(servings_sold)) %>%  
  group_by(quarter, market_id, mushy) %>%  
  mutate(segment_market_size = sum(firm_sales),  
         firm_market_share = firm_sales / segment_market_size) %>%
  ungroup()

#Compute HHI for Each Segment

data_ms <- data_ms %>%
  group_by(quarter, market_id, mushy) %>%
  summarise(HHI = sum((firm_market_share*100)^2),  .groups = "drop")

hhi_comparison <- data_ms %>%
  pivot_wider(names_from = mushy, values_from = HHI, names_prefix = "HHI_") %>%
  mutate(HHI_Difference = HHI_1 - HHI_0)  # Difference between mushy and crunchy segments

summary_hhi <- hhi_comparison %>%
  summarise(
    avg_HHI_Mushy = mean(HHI_1, na.rm = TRUE),
    avg_HHI_Crunchy = mean(HHI_0, na.rm = TRUE),
    avg_HHI_Difference = mean(HHI_Difference, na.rm = TRUE)
  )

summary_hhi


```
These results contradict the initial hypothesis that the mushy segment has a greater availability of
substitutes or a more competitive market structure. Instead, they suggest an alternative explanation:
consumers in the budget-oriented mushy segment may switch to other food options, perceiving it as nonessential
(or still can switch to the alternatives, there are 7 of them per product), while consumers in the
crunchy (higher-quality) segment demonstrate greater brand loyalty and lower price sensitivity. Additionally,
we observe that quantities sold for mushy cereals are, lower than for crunchy cereals (990 vs 1723
millions servings sold ). This may indicate weaker consumer demand for mushy cereals, further supporting
the argument that mushy cereals are perceived as less essential and more substitutable.
Additionally, the demand plot for Firm 3 (Figure 4) suggests that crunchy cereals tend to have higher
average prices, as indicated by the concentration of blue dots (crunchy products) in a higher price range
compared to red dots (mushy products). This pattern may reflect a premium positioning strategy for
crunchy cereals or a higher willingness to pay among consumers in this segment.




Q2 Compute market shares

To transform observed quantities qjt into market shares sjt = qjt/Mt, we first need to define a
market size Mt. We’ll assume that the potential number of servings sold in a market is the city’s
total population multiplied by 90 days in the quarter. Create a new variable market_size equal to
city_population times 90. Note that this assumption is somewhat reasonable but also somewhat
arbitrary.
Next, compute a new column market_share equal to servings_sold divided by market_size.
This gives our market shares sjt. We’ll also need the outside share s0t = 1 − Pj∈Jt sjt. Create a
new column outside_share equal to this expression.

```{r}

# market_size
cereal_data$market_size <- cereal_data$city_population * 90

# market_share
cereal_data$market_share <- cereal_data$servings_sold / cereal_data$market_size

# outside_share
cereal_data <- cereal_data %>%
  group_by(market) %>%
  mutate(outside_share = 1 - sum(market_share)) %>%
  ungroup()

# Stats of new vars
summary(cereal_data$market_size)
summary(cereal_data$market_share)
summary(cereal_data$outside_share)
```
Q3. Estimate the pure logit model with OLS
Recall the pure logit estimating equation: log(sjt/s0t) = δjt = αpjt+x′
jtβ +ξjt. First, create a new
column logit_delta equal to the left-hand side of this expression.
Then, run an OLS regression of logit_delta on a constant, mushy, and price_per_serving.
(a) Interpret your estimates.
(b) How can you re-express your estimate on mushy in terms of how much consumers are willing
to pay for mushy, using your estimated price coefficient?
(c) Discuss which type of standard errors you should use. Compare their estimates.

```{r}

# logit_delta
cereal_data$logit_delta <- log(cereal_data$market_share / cereal_data$outside_share)

# OLS regression
logit_model <- lm(logit_delta ~  mushy + price_per_serving, data = cereal_data)

# Print the summary of the regression
summary(logit_model)
```
(a) Interpret your estimates.
Ans:
Our model is in the form:
- logit_delta = β₀ + β₁*mushy + β₂*price_per_serving + ε
The intercept is negative (-2.93450 ) which shows that for non-mushy cereals with zero price, consumers will 
prefer the outside good. There are likely unobserved characteristics explaining this. The mushy coefficient (0.07476)
implies that a cereal being mushy increases the log-odds of it being chosen over the outside option by appx.
0.075. However it is not statistically significant given the p-value (0.16).

The price_per_serving coefficient aligns with the basic law of demand. A one unit increase in price_per_serving
reduces the log-odds of choosing that cereal by 7.49, all things being equal. This is highly significant.

Only 3.42% (R-Squared) of the variation in logit_delta is explained by the model. This is small. mushy and price_per_serving alone are not strong factors influencing the demand for cereal.  

(b) How can you re-express your estimate on mushy in terms of how much consumers are willing
to pay for mushy, using your estimated price coefficient?
Ans:
- WTP for mushy can be estimated by dividing the mushy coefficient by the price coefficient
WTP_mushy =  0.07476 / 7.48014 = 0.01. 
# Consumers are willing to pay $0.01 per serving for mushy cereal over non mushy cereal. But mushy not signiifcant

(c) Discuss which type of standard errors you should use. Compare their estimates.

- Clustered robust standard errors are preferred given that we have observations that are nested within markets. There could be unobserved factors that account for cereal demand within markets.
For the intercept, mushy1 and price_per_serving, the Cluster_Robust_SE is noticeably smaller in all cases. However the p-value for mushy improved, getting closer to the 10% significance level but still not statistically significant.

# Kindly Review this section


```{r}

# Default OLS Standard Errors (Assumes Homoscedasticity)
summary(logit_model)
summary_model <- summary(logit_model)
default_se <- summary_model$coefficients[, "Std. Error"]
coefficients <- summary_model$coefficients[, "Estimate"] # Extract coefficients as well

# Cluster-Robust Standard Errors (clustering by 'market')
cluster_vcov <- vcovCL(logit_model, cluster = ~ market) # Cluster by 'market' variable
cluster_se <- sqrt(diag(cluster_vcov))
coeftest(logit_model, vcov = cluster_vcov)

# Heteroscedasticity-Robust Standard Errors (HC3 as an example)
robust_vcov_hc3 <- vcovHC(logit_model, type = "HC3")
robust_se_hc3 <- sqrt(diag(robust_vcov_hc3))
# coeftest provides coefficient table with robust SEs and p-values
coeftest(logit_model, vcov = robust_vcov_hc3)

# Table
se_comparison_table <- data.frame(
  Variable = rownames(summary_model$coefficients),
  Coefficient = coefficients,
  Default_SE = default_se,
#  HC3_Robust_SE = robust_se_hc3,
  Cluster_Robust_SE = cluster_se
)

print(se_comparison_table, row.names = FALSE, digits = 5)


```

4. Add market and product fixed effects
Since we expect price pjt to be correlated with unobserved product quality ξjt, we should be
worried that our estimated ˆα on price is biased. Since we have multiple observations per market
and product, and prices vary both across and within markets, it is feasible for us to add both
market and product fixed-effects.
If ξjt = ξj +ξt +Δξjt and most of the correlation between pjt and ξjt is due to correlation between
pjt and either ξj (product fixed effects) or ξt (market fixed effects), then explicitly accounting for
these fixed effects during estimation should help reduce the bias of our ˆα.
Estimate a specification incorporating these fixed-effects and report a table comparing it with the
specification without fixed-effects. Interpret any changes.

Ans:
The intercept becomes positive after adding fixed effects. The market and product fixed effects indeed capture significant variation that was previously absorbed into the intercept and error term in the no-fixed-effects model. The cluster-robust standard error for the intercept increases from 0.1071 to 0.1578. 

mushy1 changes from 0.0748 in the no-FE model to -2.0889 in the FE model. The sign flips with larger absolute value that is highly statistically significant. All things being equal, consumers strongly dislike cereals that become mushy in milk. This makes sense. Mushy makes up appx 1/3 of the cereal market. The mean price_per_serving for mushy cereals is lower than for non-mushy (implies mushy cereals are cheaper). and the mean servings_sold for mushy is higher than non-mushy cereal (higher demand). 
The significant change in the mushy coefficient suggests strong omitted variable bias in the model without FE. The "mushy" characteristic was likely correlated with some omitted market or product-specific factors that were biasing the coefficient upwards (making it appear positive when the true within-market and within-product effect is negative).

#Perhaps mushy cereals were disproportionately sold in markets with systematically higher demand for cereals overall, or mushy cereals tended to be certain types of products that had other appealing characteristics not controlled for.

price_per_serving becomes much more negative, changing from -7.4801 to -28.6179. Substantially larger magnitude in the fixed effects model.The cluster-robust standard error also increases in FE. Price elasticity of demand for cereals is estimated to be considerably more elastic. Consumers are much more sensitive to price changes within the same market and for the same product in the FE model than the non-FE model.

Model with FE is much more credible as it explains over 50% of the variation in the demand for cereal compared to the 3% of the non FE model. Controlling for unobserved heterogeneity significantly improved our model


```{r}

# OLS regression with market and product fixed effects:
logit_model_fe <- lm(logit_delta ~ price_per_serving + mushy + factor(market) + factor(product), data = cereal_data)

ols_fe_summary <- summary(logit_model_fe)
ols_fe_summary

common_variables <- c("(Intercept)", "mushy1", "price_per_serving")

# Model WITHOUT FE
coef_no_fe <- coef(logit_model)[common_variables]
cluster_vcov <- vcovCL(logit_model, cluster = ~ market)
cluster_se <- sqrt(diag(cluster_vcov))[common_variables]

# Model WITH FE
coef_fe <- coef(logit_model_fe)[common_variables]
cluster_vcov_fe <- vcovCL(logit_model_fe, cluster = ~ market)
cluster_se_fe <- sqrt(diag(cluster_vcov_fe))[common_variables] 


# Comparison table
model_comparison_table <- data.frame(
  Coefficient_No_FE = coef_no_fe,
  Cluster_Robust_SE_No_FE = cluster_se,
  Coefficient_FE = coef_fe,
  Cluster_Robust_SE_FE = cluster_se_fe
)

print(model_comparison_table, row.names = FALSE, digits = 5)

```

```{r}

#OLS regression with market and product fixed effects:
logit_model_fe <- feols(logit_delta ~ price_per_serving + mushy | market + product, data = cereal_data)
logit_model_fe$collin.var
summary(logit_model_fe)
```




# Detour showing mushy summary stats.

```{r}

# Assuming your data frame is named cereal_data and you have run the summary(cereal_data) already

# Convert 'mushy' to numeric for calculating mean if it's still a factor (although summary() would give frequencies if it's a factor)
cereal_data$mushy_numeric <- as.numeric(as.character(cereal_data$mushy))

# Calculate summary statistics by mushy vs. non-mushy

mushy_summary <- cereal_data %>%
  group_by(mushy) %>%
  summarize(
    # For mushy (categorical): Frequency counts
    count = n(),

    # For servings_sold (numeric): Mean, Min, Max
    servings_sold_mean = mean(servings_sold, na.rm = TRUE),
    servings_sold_min = min(servings_sold, na.rm = TRUE),
    servings_sold_max = max(servings_sold, na.rm = TRUE),

    # For city_population (numeric): Mean, Min, Max (though city_population might be constant in your sample data)
    city_population_mean = mean(city_population, na.rm = TRUE),
    city_population_min = min(city_population, na.rm = TRUE),
    city_population_max = max(city_population, na.rm = TRUE),

    # For price_per_serving (numeric): Mean, Min, Max
    price_per_serving_mean = mean(price_per_serving, na.rm = TRUE),
    price_per_serving_min = min(price_per_serving, na.rm = TRUE),
    price_per_serving_max = max(price_per_serving, na.rm = TRUE),

    # For price_instrument (numeric): Mean, Min, Max
    price_instrument_mean = mean(price_instrument, na.rm = TRUE),
    price_instrument_min = min(price_instrument, na.rm = TRUE),
    price_instrument_max = max(price_instrument, na.rm = TRUE)
  )

# Print the summary table
print(mushy_summary)


```

# Detour exploring firm effect. Kindly ignore

```{r}

cereal_data$firm_id <- substr(cereal_data$product, 1, 2)
cereal_data$firm_id <- as.factor(cereal_data$firm_id)

cereal_data$market_id <- substr(cereal_data$market, 1, 3)
cereal_data$market_id <- as.factor(cereal_data$market_id)

cereal_data$quarter <- stringr::str_sub(cereal_data$market, start = -2) # Negative start index counts from the end
cereal_data$quarter <- as.factor(cereal_data$quarter)


logit_model_firm_last3_fe <- lm(logit_delta ~ price_per_serving + mushy +  factor(market) + factor(product)+ factor(quarter) + factor(market_id) + factor(firm_id), data = cereal_data)

summary(logit_model_firm_last3_fe)

head(cereal_data)

```

5. Add an instrument for price
Adding market and product fixed effects can be helpful, but since unobserved quality typically
varies by both product and market, we really want to instrument for prices. The data comes with
a column price_instrument that we should interpret as a valid instrument for price that satisfies
the needed exclusion restriction. It could be a cost-shifter, a valid Hausman instrument, or similar.

(a) Run a first-stage regression to make sure that it’s a relevant instrument for price. Does
price_instrument seem like a relevant instrument for prices?

Ans:
Coefficient on price_instrument is highly Significant, further portrayed by the large t-value (151.181) and very small p-value (< 2e-16). The price_instrument is strongly correlated with price_per_serving in the first-stage regression.
The coefficient is positive (0.8771), showing that it is indeed a cost-shifter which in the economic sense should be positively correlated with price. A very high R-squared of over 96% shows that our first-stage model explains a large portion of the variation in price_per_serving. Instrument seems to be relevant. 

(b) Run your IV regression using price_instrument as an instrument for price_per_serving.
How does your estimate of ˆα change?

Ans:
#  Larger cluster-robust standard errors, compared to the homoscedastic ones, indicate that accounting for within-market correlation does impact the precision of estimates.
OLS (FE Model) Coefficient for price_per_serving:  -28.61787 
IV (2SLS) Coefficient for price_per_serving:  -30.59952

The fact that the IV coefficient is more negative than the OLS coefficient suggests that the OLS estimate of the price effect might have been biased towards zero (less negative) due to endogeneity. This is indicative of positive endogeneity bias in the OLS model. IV estimation improved the estimate by reducing the bias, leading to a more negative and (hopefully) less biased estimate of the true price effect. IV shows demand for cereal is more elastic as shown in OLS.


```{r}

# First-Stage Regression for Instrument Relevance 

# First-stage regression: IV
first_stage_model <- lm(price_per_serving ~ price_instrument + mushy + factor(market) + factor(product), data = cereal_data)

# Summarize the first-stage regression
summary(first_stage_model)

# Extract and print the coefficient and t-statistic for price_instrument
first_stage_summary <- summary(first_stage_model)
price_instrument_coef <- first_stage_summary$coefficients["price_instrument", "Estimate"]
price_instrument_t_value <- first_stage_summary$coefficients["price_instrument", "t value"]
price_instrument_p_value <- first_stage_summary$coefficients["price_instrument", "Pr(>|t|)"]

cat("First-Stage Regression - Relevance of price_instrument:\n")
cat("Coefficient for price_instrument: ", price_instrument_coef, "\n")
cat("t-value for price_instrument: ", price_instrument_t_value, "\n")
cat("p-value for price_instrument: ", price_instrument_p_value, "\n\n")

# IV Regression (Second Stage)

# IV Regression using ivreg(), instrumenting price_per_serving with price_instrument
iv_model <- ivreg(logit_delta ~ price_per_serving + mushy + factor(market) + factor(product) |
                    price_instrument + mushy + factor(market) + factor(product),
                  data = cereal_data)

# Summarize the IV regression + Homoscedastic SE 
summary(iv_model)

# Summarize the IV regression + Cluster Robust SE
iv_summary <- coeftest(iv_model, vcov = vcovCL(iv_model, cluster = ~ market)) 
print(iv_summary)


iv_intercept <- iv_summary["(Intercept)", "Estimate"]
iv_intercept_se <- iv_summary["(Intercept)", "Std. Error"]
iv_price_coef <- iv_summary["price_per_serving", "Estimate"]
iv_price_se <- iv_summary["price_per_serving", "Std. Error"]

cat("IV Regression Results (Cluster-Robust Standard Errors):\n")
cat("IV Coefficient for price_per_serving: ", iv_price_coef, "\n")
cat("Cluster-Robust Standard Error for price_per_serving: ", iv_price_se, "\n")

# Compare IV estimate with OLS estimate (from FE model)
# (Assuming you have already run and stored the FE model results in 'logit_model_fe' and 'coeftest_fe')
cluster_vcov_fe <- vcovCL(logit_model_fe, cluster = ~ market)
coeftest_fe <- coeftest(logit_model_fe, vcov = cluster_vcov_fe)

ols_fe_intercept <- ols_fe_summary$coefficients["(Intercept)", "Estimate"]
ols_fe_intercept_se <- ols_fe_summary$coefficients["(Intercept)", "Std. Error"]
ols_fe_summary <- coeftest_fe # coeftest_fe contains cluster-robust SEs for FE model from previous steps
ols_fe_price_coef <- ols_fe_summary["price_per_serving", "Estimate"]
ols_fe_price_se <- ols_fe_summary["price_per_serving", "Std. Error"]

cat("\nComparison of Price Coefficients:\n")
cat("OLS (FE Model) Coefficient for price_per_serving: ", ols_fe_price_coef, "\n")
cat("OLS (FE Model) Cluster-Robust Standard Error: ", ols_fe_price_se, "\n")
cat("IV (2SLS) Coefficient for price_per_serving: ", iv_price_coef, "\n")
cat("IV (2SLS) Cluster-Robust Standard Error: ", iv_price_se, "\n")

cat("OLS (FE Model) Coefficient for Intercept: ", ols_fe_intercept, "\n")
cat("OLS (FE Model) Cluster-Robust Intercept Standard Error: ", ols_fe_intercept_se, "\n")
cat("IV (2SLS) Coefficient for Intercept: ", iv_intercept, "\n")
cat("IV (2SLS) Cluster-Robust Intercept Standard Error: ", iv_intercept_se, "\n")



```

6. Promotion analysis
(a) With the estimates obtained above, compute the own-price elasticities of demand and associated
price-cost margins. Discuss their credibility

Note: Use ηjj = −αpj(1 − sj), as per slides.
Next, your task is to simulate what would happen if we halved an important product’s price? To
do so, select the most recent quarter in the first city: C01Q2.
Create a new dataframe called counterfactual_data for just that market and inspect the data.
We’ll pretend that we’re firm one, and deciding whether we want to cut the price of our brand
fourth’s product F1B04.
In your new dataframe with just data from C01Q2, create a new_prices column that is the same
as prices but with the price of F1B04 cut in half.

(b) Compute the percent change in sales of each product in the market. What can you say about
the price elasticity of demand of the products?

Ans-
Halving the price of F1B04 leads to a more than proportionate increase in its market share (223.64%) given its high price elasticity (-2.362803). 
The substitution effect led to a uniform decrease in the market shares of competing products by 1.45% even though they all have varying own price elasticities. CHECK: This uniform decreace is probably due to limitation of the logit model ?

(c) From firm one’s perspective, do the estimates of product cannibalization make sense? That
is, do the signs and magnitudes on the demand responses to the price change make sense?

Ans-
No. One would expect products closer in characteristic space to be closer substitutes and such be more impacted by the change in price of F1B04. But that is not the case here since our logit model is not a function of product characteristics, which is limitation of the logit model. Substitution occurs uniformly across competing product market shares. Overall, only signs make sense in this estimation.  


```{r}

# First City Data C01Q2
counterfactual_data <- cereal_data[cereal_data$market == "C01Q2", ]
# counterfactual_data2 <- subset(cereal_data, market == "C01Q2")
counterfactual_data <- counterfactual_data %>%
  select(
    product,     
    mushy,       
    price_per_serving,
    market_share,
    logit_delta
  )

summary(counterfactual_data)
head(counterfactual_data)
# summary(counterfactual_data2)

# Halving price_per_serving for F1B04
counterfactual_data$new_prices <- counterfactual_data$price_per_serving
product_to_cut_price <- "F1B04"
counterfactual_data$new_prices[counterfactual_data$product == product_to_cut_price] <-
  counterfactual_data$new_prices[counterfactual_data$product == product_to_cut_price] / 2

# Validate
print(counterfactual_data[counterfactual_data$product == product_to_cut_price, c("product", "price_per_serving", "new_prices")])

# Extract α from the FE logit model (you can also replace with IV model) 
# Needed to calculate change given η formula: η_jj = −αp_j(1 − s_j)
alpha <- -coef(iv_model)["price_per_serving"]

# Calculate the own-price elasticity for each product 
counterfactual_data <- counterfactual_data %>%
  mutate(own_price_elasticity = -alpha * price_per_serving * (1 - market_share))

# Calculate the new logit_delta for F1B04
counterfactual_data <- counterfactual_data %>%
  mutate(logit_delta_new = ifelse(product == "F1B04", logit_delta - alpha * (new_prices - price_per_serving), logit_delta))

# Calculate the new market shares
total_logit <- sum(exp(counterfactual_data$logit_delta_new))
counterfactual_data <- counterfactual_data %>%
  mutate(new_market_share = exp(logit_delta_new) / (1 + total_logit))

# Calculate the percent change in market shares
counterfactual_data <- counterfactual_data %>%
  mutate(percent_change_market_share = (new_market_share - market_share) / market_share * 100)

# Inspect the final counterfactual data
print(counterfactual_data %>% select(product, price_per_serving, new_prices, market_share, new_market_share, own_price_elasticity, percent_change_market_share))


# Analyze the results
cat("Own-Price Elasticity for F1B04:", counterfactual_data %>% filter(product == "F1B04") %>% pull(own_price_elasticity), "\n")
cat("Percent Change in Market Share for F1B04:", counterfactual_data %>% filter(product == "F1B04") %>% pull(percent_change_market_share), "\n")


```





```{r}

```



```{r}


```